{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a2b896-7de6-4abe-a85c-d7f46203d74c",
   "metadata": {},
   "source": [
    "### Set the variables for base model, dataset, and new model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc623d-0150-4ffa-8923-4d8e2d8ef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9ac95-70b6-4ffe-9694-0a2132266d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_loc = \"output/checkpoints\"\n",
    "lora_adapter_loc = \"output/lora_adapter\"\n",
    "#dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b834150-1f0a-491b-809b-f1082af0779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLEARML_LOG_MODEL=True\n",
    "from clearml import Task, OutputModel, Dataset\n",
    "\n",
    "project_name = 'userxx/CustomerSupport'\n",
    "task_name = \"03-fine-tune-llama3-2-1b\"\n",
    "s3_base_bucket_loc = 's3://tk-aip/clearml'\n",
    "base_model = \"/mnt/shared/models/huggingface/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "task = Task.init(project_name=project_name, task_name=task_name, output_uri=s3_base_bucket_loc)\n",
    "#Task.add_requirements(\"./requirements.txt\")\n",
    "#task.set_parameter(\"base-model\",base_model)\n",
    "#task.set_parameter(\"lora-adapter\",task_name)\n",
    "#task.set_base_docker(docker_image=\"nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\")\n",
    "#task.set_base_docker(docker_image=\"nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04\")\n",
    "#task.execute_remotely(queue_name=\"q-group-a-gpu-10gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c3383-f7b9-4ec2-9b76-716e0018c984",
   "metadata": {},
   "source": [
    "### Load the Python packages and functions we will use throughout the fine-tuning and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec051a7-9617-405e-95b1-b6470858dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f54975-0315-46e2-903c-73f3a61d9247",
   "metadata": {},
   "source": [
    "## 2. Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492aff83-e30f-4d10-a129-56b6a314febb",
   "metadata": {},
   "source": [
    "### Set the data type and attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca092f-7c00-4352-907f-78e62631e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch dtype and attention implementation\n",
    "#if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#    print(\"install flash-attn!!!\")\n",
    "##    !pip install -qqq flash-attn\n",
    "##    !pip install flash-attn --no-build-isolation\n",
    "#    torch_dtype = torch.bfloat16\n",
    "#    attn_implementation = \"flash_attention_2\"\n",
    "#else:\n",
    "#    torch_dtype = torch.float16\n",
    "#    attn_implementation = \"eager\"\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ae918-58d5-4bd9-be07-0342882a134d",
   "metadata": {},
   "source": [
    "### Load the model and tokenizer by providing the local model directory, we will load the model in 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7e369-663c-48a1-8693-bfcb5616b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded953b-4658-4e9f-a8e8-86e010dd174d",
   "metadata": {},
   "source": [
    "## 3. Loading and processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c72a6a-901d-4421-83fb-5770e9d93633",
   "metadata": {},
   "outputs": [],
   "source": [
    "cml_dataset_project = project_name\n",
    "cml_dataset_name = \"bitext-customer-support\"\n",
    "cml_dataset = Dataset.get(\n",
    "        dataset_project=cml_dataset_project,\n",
    "        dataset_name=cml_dataset_name,\n",
    "        only_completed=True,\n",
    "        only_published=False,\n",
    ")\n",
    "dataset_path = cml_dataset.get_local_copy()\n",
    "files = cml_dataset.list_files()\n",
    "data_file = dataset_path + \"/\" + files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8940615-1e15-428b-8c9a-d2e53878b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "#dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = load_dataset(\"csv\", data_files=data_file, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97faa27a-cee6-4b44-bda9-e752eeb3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'][3]\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fb533-6b1e-44d3-93ad-7616e957ba55",
   "metadata": {},
   "source": [
    "## 4. Setting up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3e438-4d99-4a2b-b487-95a05aef4680",
   "metadata": {},
   "source": [
    "### Extract the linear model name from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca6a4a-5dd4-4388-95a8-bb7e534711bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "#modules = ['gate_proj', 'v_proj', 'up_proj', 'q_proj', 'down_proj', 'k_proj', 'o_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d570be8-986a-4795-a5b2-3d2641f18fba",
   "metadata": {},
   "source": [
    "### Use the linear module name to create the LoRA adopter. We will only fine-tune the LoRA adopter and leave the rest of the model to save memory and for faster training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e582a89-2f36-487f-9f5b-ce73c30431fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54d4e2-b769-4f2e-9bc8-238c0409bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=checkpoint_loc,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "#    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de896a-6b38-4c0b-8d04-66507b7a5e43",
   "metadata": {},
   "source": [
    "### We will now set up a supervised fine-tuning (SFT) trainer and provide a train and evaluation dataset, LoRA configuration, training argument, tokenizer, and model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721daa56-64b1-4eaf-a858-231c36a6a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "#    max_seq_length= 512,\n",
    "#    dataset_text_field=\"text\",\n",
    "#    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "#    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f55aa4-76d3-4f55-907a-8753d120e425",
   "metadata": {},
   "source": [
    "## 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f601193-f826-4556-9c0f-0fd6a52fb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c594c-3b07-4afd-b5bd-8cde56d98d7e",
   "metadata": {},
   "source": [
    "## 6. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdb429-6666-4865-b00b-ed16c070d633",
   "metadata": {},
   "source": [
    "### To test the fine-tuned model, we will provide it with the sample prompt from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1103f1-5558-46db-b458-4ae30f3f43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcfbf1-0fb7-4ec6-af78-7dfba707d7b5",
   "metadata": {},
   "source": [
    "## 7. Saving the tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd5ec7-036a-4da0-9030-64b468c3d3a2",
   "metadata": {},
   "source": [
    "### Output LoRA Adapter to local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49200aca-d34c-425c-b474-153ffb9a1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(lora_adapter_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e7c1e-ca9b-4a91-9f0f-6a816f12f2be",
   "metadata": {},
   "source": [
    "### Upload LoRA Adapter in Zipped format to S3 Bucket via ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c3aa1-8a33-45d9-b12d-56c1ae71d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, OutputModel, StorageManager\n",
    "\n",
    "s3_bucket_loc = s3_base_bucket_loc + \"/\" + Task.current_task().get_project_name() + \"/\" + task_name + \".\" + task.id + \"/models/lora_adapter\"\n",
    "StorageManager.upload_folder(lora_adapter_loc, s3_bucket_loc)\n",
    "clearmlModel = OutputModel(\n",
    "    task=Task.current_task(),\n",
    "    framework=\"PyTorch\",\n",
    "    name=\"remote_lora_adapter_loc\"\n",
    ")\n",
    "clearmlModel.update_weights(\n",
    "    register_uri=s3_bucket_loc,\n",
    "    target_filename = \"remote_lora_adapter_loc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dcc71-f16f-43d0-960b-0882a2c2cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora_adapter = task_name\n",
    "\n",
    "#from clearml import Task, OutputModel\n",
    "# Upload the merged model to S3 bucket\n",
    "clearmlModel = OutputModel(\n",
    "    task=Task.current_task(),\n",
    "    framework=\"PyTorch\",\n",
    "    name=\"remote_lora_adapter_zipped\"\n",
    ")\n",
    "clearmlModel.update_weights_package(\n",
    "    weights_path=lora_adapter_loc,\n",
    "    target_filename=\"remote_lora_adapter_zipped\",\n",
    "    auto_delete_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4100384-b56c-4669-98c2-3b0d7353150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93ef86-65ed-471d-9d79-767b28f41e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
