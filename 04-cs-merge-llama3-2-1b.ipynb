{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b834150-1f0a-491b-809b-f1082af0779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, OutputModel\n",
    "\n",
    "project_name = 'userxx/CustomerSupport'\n",
    "task_name = \"04-merge-lora-adapter-with-base-model\"\n",
    "s3_base_bucket_loc = 's3://tk-aip/clearml'\n",
    "base_model = \"/mnt/shared/models/huggingface/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "task = Task.init(project_name=project_name, task_name=task_name, output_uri=s3_base_bucket_loc)\n",
    "#Task.add_requirements(\"./requirements.txt\")\n",
    "#task.set_base_docker(docker_image=\"nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04\")\n",
    "#task.execute_remotely(queue_name=\"q-group-a-gpu-10gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944358ea-f2d0-492c-a369-9d86ebd71a5e",
   "metadata": {},
   "source": [
    "## 1. Download LoRA Adapter from the Previous ClearML Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be32fd-0a50-4da3-b704-10e7291e808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousTaskId = task.get_parameter(\"General/previous_task_id\")\n",
    "if (previousTaskId is not None):\n",
    "    print(\"General/previous_task_id is available, previousTaskId=\" + previousTaskId)\n",
    "    previousTask = Task.get_task(task_id = previousTaskId)\n",
    "else:\n",
    "    previous_task_name = \"03-fine-tune-llama3-2-1b\"\n",
    "    print(\"General/previous_task_id is NOT available, use previous_task_name=\" + previous_task_name)\n",
    "    previousTask = Task.get_task(project_name=project_name, task_name=previous_task_name)\n",
    "    \n",
    "if (previousTask is not None):\n",
    "    models = previousTask.get_models().get(\"output\")\n",
    "    for model in models:\n",
    "        print(model.name)\n",
    "        if (model.name==\"remote_lora_adapter_zipped\"):\n",
    "        #if (model.name.startswith(\"checkpoint-\")):\n",
    "            lora_adapter_loc = model.get_local_copy(extract_archive=True)\n",
    "            print(\"lora_adapter_loc = \" + lora_adapter_loc)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84eb9a4-9922-42ed-b0d1-ae96518b7d3b",
   "metadata": {},
   "source": [
    "## 2. Merge Base Model wit LoRA Adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbfd70-c130-4598-a99f-4325a96de5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/mnt/shared/models/huggingface/Llama-3.2-1B-Instruct\"\n",
    "#lora_adapter_loc = \"output/lora_adapter\"\n",
    "merged_model_loc = \"output/merged_model\"\n",
    "remote_merged_model_loc = \"/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2781c-2791-421a-aa0c-88d52598dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from trl import setup_chat_format\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d1f7e-644c-4763-ab29-d2e6be6ec74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, lora_adapter_loc)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a038079-af2c-4d40-bf50-545ec03d49d9",
   "metadata": {},
   "source": [
    "## 3. Merged-Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da386a29-806f-4387-b95f-31bcb99cd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I have to see what payment payment modalities are accepted\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b1c4f-28c2-44d0-b4f9-08537d045330",
   "metadata": {},
   "source": [
    "## 3. Save the tokenizer and model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2054da-66d1-4f0a-aa8e-1c585da87d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(merged_model_loc)\n",
    "tokenizer.save_pretrained(merged_model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cdd2b-3d57-4818-a24d-adccc2d746af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(remote_merged_model_loc)\n",
    "tokenizer.save_pretrained(remote_merged_model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f9bcc-d361-4051-bd7d-1aa132ae424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora_adapter = task_name\n",
    "\n",
    "#from clearml import Task, OutputModel\n",
    "# Upload the merged model to S3 bucket\n",
    "clearmlModel = OutputModel(\n",
    "    task=Task.current_task(),\n",
    "    framework=\"PyTorch\",\n",
    "    name=\"llama-3.2-1b-instruct-customerservice\"\n",
    ")\n",
    "clearmlModel.update_weights_package(\n",
    "    weights_path=remote_merged_model_loc,\n",
    "    target_filename=\"merge_model_zipped\",\n",
    "    auto_delete_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1d4c3-de84-4112-b392-6905cb45ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c8897-aca2-49db-88f2-2fdfc73164c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
