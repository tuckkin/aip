{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b834150-1f0a-491b-809b-f1082af0779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 04:00:59,182 - clearml - WARNING - InsecureRequestWarning: Certificate verification is disabled! Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "ClearML Task: created new task id=6fb9d528578a453882d2b3f77f0c9bb1\n",
      "2025-07-26 04:00:59,785 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clearml.bizilife.net/projects/d4b25b0d2de64d6bb07fbca370f0439e/experiments/6fb9d528578a453882d2b3f77f0c9bb1/output/log\n",
      "ClearML results page: https://app.clearml.bizilife.net/projects/d4b25b0d2de64d6bb07fbca370f0439e/experiments/6fb9d528578a453882d2b3f77f0c9bb1/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task, OutputModel\n",
    "\n",
    "project_name = 'cnasg-tk/CustomerSupport'\n",
    "task_name = \"04-merge-lora-adapter-with-base-model\"\n",
    "s3_base_bucket_loc = 's3://tk-aip/clearml'\n",
    "base_model = \"/mnt/shared/models/huggingface/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "task = Task.init(project_name=project_name, task_name=task_name, output_uri=s3_base_bucket_loc)\n",
    "#Task.add_requirements(\"./requirements.txt\")\n",
    "#task.set_base_docker(docker_image=\"nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04\")\n",
    "#task.execute_remotely(queue_name=\"q-group-a-gpu-10gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944358ea-f2d0-492c-a369-9d86ebd71a5e",
   "metadata": {},
   "source": [
    "## 1. Download LoRA Adapter from the Previous ClearML Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17be32fd-0a50-4da3-b704-10e7291e808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General/previous_task_id is NOT available, use previous_task_name=03-fine-tune-llama3-2-1b\n",
      "2025-07-26 04:01:00,718 - clearml.util - WARNING - 5 task found when searching for `{'project_name': 'cnasg-tk/CustomerSupport', 'task_name': '03-fine-tune-llama3-2-1b', 'include_archived': True}`\n",
      "2025-07-26 04:01:00,719 - clearml.util - WARNING - Selected task `03-fine-tune-llama3-2-1b` (id=4f13e3616926496281180d1f83ddfa6a)\n",
      "03-fine-tune-llama3-2-1b - training_args\n",
      "03-fine-tune-llama3-2-1b - optimizer\n",
      "03-fine-tune-llama3-2-1b - scheduler\n",
      "03-fine-tune-llama3-2-1b - rng_state\n",
      "checkpoint-450\n",
      "remote_lora_adapter_loc\n",
      "remote_lora_adapter_zipped\n",
      "2025-07-26 04:01:02,447 - clearml.storage - INFO - Downloading: 43.03MB from s3://tk-aip/clearml/cnasg-tk/CustomerSupport/03-fine-tune-llama3-2-1b.4f13e3616926496281180d1f83ddfa6a/models/remote_lora_adapter_zipped.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "███████████████████████████████ 100% | 43.03/43.03 MB [00:01<00:00, 40.43MB/s]: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 04:01:03,521 - clearml.storage - INFO - Downloaded 43.03 MB successfully from s3://tk-aip/clearml/cnasg-tk/CustomerSupport/03-fine-tune-llama3-2-1b.4f13e3616926496281180d1f83ddfa6a/models/remote_lora_adapter_zipped.zip , saved to /root/.clearml/cache/storage_manager/global/5b55e4e1efc27a6861d45f22694b7a8d.remote_lora_adapter_zipped.zip\n",
      "lora_adapter_loc = /root/.clearml/cache/storage_manager/global/5b55e4e1efc27a6861d45f22694b7a8d.remote_lora_adapter_zipped_artifacts_archive_None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "previousTaskId = task.get_parameter(\"General/previous_task_id\")\n",
    "if (previousTaskId is not None):\n",
    "    print(\"General/previous_task_id is available, previousTaskId=\" + previousTaskId)\n",
    "    previousTask = Task.get_task(task_id = previousTaskId)\n",
    "else:\n",
    "    previous_task_name = \"03-fine-tune-llama3-2-1b\"\n",
    "    print(\"General/previous_task_id is NOT available, use previous_task_name=\" + previous_task_name)\n",
    "    previousTask = Task.get_task(project_name=project_name, task_name=previous_task_name)\n",
    "    \n",
    "if (previousTask is not None):\n",
    "    models = previousTask.get_models().get(\"output\")\n",
    "    for model in models:\n",
    "        print(model.name)\n",
    "        if (model.name==\"remote_lora_adapter_zipped\"):\n",
    "        #if (model.name.startswith(\"checkpoint-\")):\n",
    "            lora_adapter_loc = model.get_local_copy(extract_archive=True)\n",
    "            print(\"lora_adapter_loc = \" + lora_adapter_loc)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84eb9a4-9922-42ed-b0d1-ae96518b7d3b",
   "metadata": {},
   "source": [
    "## 2. Merge Base Model wit LoRA Adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fbfd70-c130-4598-a99f-4325a96de5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/mnt/shared/models/huggingface/Llama-3.2-1B-Instruct\"\n",
    "#lora_adapter_loc = \"output/lora_adapter\"\n",
    "merged_model_loc = \"output/merged_model\"\n",
    "remote_merged_model_loc = \"/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d2781c-2791-421a-aa0c-88d52598dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from trl import setup_chat_format\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0d1f7e-644c-4763-ab29-d2e6be6ec74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, lora_adapter_loc)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a038079-af2c-4d40-bf50-545ec03d49d9",
   "metadata": {},
   "source": [
    "## 3. Merged-Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da386a29-806f-4387-b95f-31bcb99cd66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "We appreciate your inquiry about our payment options! We strive to provide a seamless and convenient payment experience for our valued customers. Our accepted payment modalities include:\n",
      "\n",
      "- Credit/Debit Card: We accept major credit and debit cards such as Visa, Mastercard, and American Express.\n",
      "- PayPal: A widely recognized and trusted online payment method.\n",
      "- Bank Transfer: Direct bank transfers to our account for secure and efficient payments.\n",
      "- Apple Pay: For Apple users, we offer a convenient and secure way to make payments.\n",
      "\n",
      "These payment modalities are available at our website and at our physical locations. If you have any specific questions or need further assistance with a particular payment method, please feel free to let us know. We're here to help!\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I have to see what payment payment modalities are accepted\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b1c4f-28c2-44d0-b4f9-08537d045330",
   "metadata": {},
   "source": [
    "## 3. Save the tokenizer and model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2054da-66d1-4f0a-aa8e-1c585da87d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/merged_model/tokenizer_config.json',\n",
       " 'output/merged_model/special_tokens_map.json',\n",
       " 'output/merged_model/chat_template.jinja',\n",
       " 'output/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(merged_model_loc)\n",
    "tokenizer.save_pretrained(merged_model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194cdd2b-3d57-4818-a24d-adccc2d746af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice/tokenizer_config.json',\n",
       " '/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice/special_tokens_map.json',\n",
       " '/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice/chat_template.jinja',\n",
       " '/mnt/shared/models/fine-tuned/merged/llama-3.2-1b-instruct-customerservice/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(remote_merged_model_loc)\n",
    "tokenizer.save_pretrained(remote_merged_model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95f9bcc-d361-4051-bd7d-1aa132ae424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 04:20:34,199 - clearml.storage - INFO - Starting upload: /tmp/model_package.irarap8l.zip => tk-aip/clearml/cnasg-tk/CustomerSupport/04-merge-lora-adapter-with-base-model.6fb9d528578a453882d2b3f77f0c9bb1/models/merge_model_zipped.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://tk-aip/clearml/cnasg-tk/CustomerSupport/04-merge-lora-adapter-with-base-model.6fb9d528578a453882d2b3f77f0c9bb1/models/merge_model_zipped.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 04:20:34,690 - clearml.storage - INFO - Uploading: 2373.61MB from /tmp/model_package.irarap8l.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   0% | 2489.61/? MB [00:22<00:00, 110.57MB/s]: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 04:20:57,211 - clearml.Task - INFO - Completed model upload to s3://tk-aip/clearml/cnasg-tk/CustomerSupport/04-merge-lora-adapter-with-base-model.6fb9d528578a453882d2b3f77f0c9bb1/models/merge_model_zipped.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#lora_adapter = task_name\n",
    "\n",
    "#from clearml import Task, OutputModel\n",
    "# Upload the merged model to S3 bucket\n",
    "clearmlModel = OutputModel(\n",
    "    task=Task.current_task(),\n",
    "    framework=\"PyTorch\",\n",
    "    name=\"llama-3.2-1b-instruct-customerservice\"\n",
    ")\n",
    "clearmlModel.update_weights_package(\n",
    "    weights_path=remote_merged_model_loc,\n",
    "    target_filename=\"merge_model_zipped\",\n",
    "    auto_delete_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1d4c3-de84-4112-b392-6905cb45ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c8897-aca2-49db-88f2-2fdfc73164c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
